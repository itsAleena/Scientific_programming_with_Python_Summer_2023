{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1921c4-f586-405a-b12c-a00f0a93acb3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-515e01b25bda86a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercises and Homework for week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895ad84-bb8f-46ac-8757-1c783e9aed80",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-14ca577e70096fbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## physics725: Scientific Programming with Python (SS 2023)\n",
    "\n",
    "Oliver Cordes & Thomas Erben\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56f1baa-51f8-4f55-bc7a-205299e425bd",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-58e8949e2e41603c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Homework is due on **Thursday, 08/06/2023, 11:55pm**\n",
    "\n",
    " * You only learn a programming language by actively praticing and using it! We therefore **strongly** advise you to work on the homework problems.\n",
    " * Please discuss the problems with your student peers and with your tutor.\n",
    " * Your code(s) need(s) to be well and appropriately commented!\n",
    " * Submit this notebook and, if necessary, additional files in a `tar`-archive with name `Homework_7_group_XX.tgz` (replace `XX` with your group number) to your tutor (in this exercise the notebook, `text_analysis.py`, `task3.py` and `letters.py`!)\n",
    "\n",
    "**Topics of this exercise:**\n",
    " * Create your own Python modul\n",
    " * Using Dictionaries and Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead84dd-3ab3-4621-af6a-28af721ba03f",
   "metadata": {},
   "source": [
    "**Your group number here please:**  Group XX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c337c82-0ac4-4981-adec-aec90dbc0441",
   "metadata": {},
   "source": [
    "## 1. Lecture Review (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd69775-ea13-407d-9b95-6de94808bfe7",
   "metadata": {},
   "source": [
    "If you did the lecture review questions [05_Review_questions.ipynb](05_Review_questions.ipynb) (strongly recommended!): \n",
    "Please discuss with your tutor and your group any issues/problems you had with them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b8f21a-5ab8-41cb-baca-b06d3366eb30",
   "metadata": {},
   "source": [
    "## 2. Text analysis (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6033c-af52-4e54-be41-431ca3a2a6ae",
   "metadata": {},
   "source": [
    "Writing texts will produce errors, some of these errors are quite simple. Typos, word duplication and punctuation. Typos can be mainly fixed by online dictionaries but for word duplication and punctuation detection special programs are needed. We want to implement a simple word duplication with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6356023d-56af-494a-949f-44be2441e57e",
   "metadata": {},
   "source": [
    "Let's assume these kind of errors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e163a-3a87-4b1c-8d5e-a640432c6758",
   "metadata": {},
   "source": [
    "``` \n",
    "... we often often make ...\n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```\n",
    "... here. this ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8b14cc-10db-4aa6-9406-c719118f3fd9",
   "metadata": {},
   "source": [
    "In `data/faulty_text.txt` we have added a few errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b6d65-675a-47c4-b57f-eec31f16ade3",
   "metadata": {},
   "source": [
    "**Your task:**\n",
    "\n",
    "Write a python script `text_analysis.py` in which you write a function `detect_errors` with takes a filename as an argument and uses the `logging`-module for indicate the `errors` (`logging.error`). Test your function with `data/faulty_text.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab34952f-702d-41f0-8e5b-01636dd31e02",
   "metadata": {},
   "source": [
    "**Hints:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f508ccf0-5a27-406a-a8b3-2f0180bf6e0d",
   "metadata": {},
   "source": [
    " * you need to perform two different tests for word duplication and punctuation per line\n",
    " * for punctuation is simple `.split` is useful\n",
    " * for word duplication all punctuations are not necessary; python provides a special modul called `re` which defines a special `split` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca398271-05e2-40d6-b79f-f9278bc79e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to split a string into words taking into account\n",
    "# (removing) punctuation.\n",
    "# For time reasons, we will not treat 'regular expressions' in class\n",
    "# but you should look them up yourself! You should know them from\n",
    "# Linux already.\n",
    "\n",
    "import re # module to handle regular expressions in a Python program\n",
    "\n",
    "s = \"Here some text with double (double!) words words. It also contains puctuation!\"\n",
    "\n",
    "# split s into its words without the punctuation marks; note that\n",
    "# you might end up with empty strings in the word list!\n",
    "words = re.split('\\W+', s.rstrip())\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafbc24-b437-4627-9674-484cbf138925",
   "metadata": {},
   "source": [
    " * take also into account that you check for duplication after switching to a new line\n",
    "   ```\n",
    "   ... test\n",
    "   \n",
    "   test ...\n",
    "   ```\n",
    "   Also do this for the punctuation tests!\n",
    " * keep also track of line numbers and word positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50bef04-0a97-4b15-a913-6c498ce47d15",
   "metadata": {},
   "source": [
    "My program gave these results:\n",
    "\n",
    "```\n",
    "ERROR:root:line 1 word #6: often\n",
    "ERROR:root:line 2 word #6: here\n",
    "ERROR:root:line 2 word #7: this\n",
    "ERROR:root:lines 5+6: words\n",
    "ERROR:root:lines 8+10: test\n",
    "ERROR:root:line 11 word #1: this\n",
    "```\n",
    "\n",
    " * it is not necessary to reproduce the results in detail, but you should address the same errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaccb025-d076-4400-bea2-ff72a7823251",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af952fcf-eacf-4e97-8461-debf8a75a352",
   "metadata": {},
   "source": [
    "## 3. Language detection of text files (25=0+10+5+5+5) points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ca6b1-a5f9-4792-bc17-776b96bc620b",
   "metadata": {},
   "source": [
    "Language detection of written texts can be very complex, but we want to implement an easy to understand solution. The analysis is based on letter frequency (see this [Wikipedia article](https://en.wikipedia.org/wiki/Letter_frequency)). So for an unknown text the letter frequency can be calculated and compared with some predefined statistics:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ccb00c-a17c-4901-b76f-a15ca0eb1e18",
   "metadata": {},
   "source": [
    "```Python\n",
    "wikipedia_stats_english = { 'e': 0.1270, 't': 0.09056, 'a': 0.08167, 'o': 0.07507, 'l': 0.06966, 'n' : 0.06749 }\n",
    "wikipedia_stats_german  = { 'e': 0.1639, 'n': 0.0978, 's': 0.0727, 'r' : 0.0700, 'i': 0.0655, 'a': 0.0651 } \n",
    "wikipedia_stats_italian = { 'e': 0.1179, 'a': 0.1174, 'i': 0.1128, 'o' : 0.0983, 'n': 0.0688, 'l': 0.0651 } \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "I simply used these three languages, since other europeen languages can be simply identified by some special characters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3099f-f8b4-4817-9676-69f7a9ced355",
   "metadata": {},
   "source": [
    "In the folder `texts` we have a few text files for which you should decide which language is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38128996-a2be-4a52-ae18-827bb11cfaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e91db-a147-42d4-b88d-b8b9170d81ad",
   "metadata": {},
   "source": [
    "### 3.1 Setup of program files (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70102cc7-a657-419d-b935-d42aeca6455b",
   "metadata": {},
   "source": [
    "For this task we want to create a Python module `letters.py` which should contain all functions of this exercise. The main script with your complete solution should be named `task3.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a822db-d12b-4a43-826a-1f578be1c636",
   "metadata": {},
   "source": [
    "**Your task:**\n",
    "\n",
    "Switch to the desktop environment of the Jupyterhub-Online-System and create the two files with an editor.\n",
    "\n",
    "**Hints:**\n",
    "In the case you don't know which editor to use, please ask immediately your tutor for support. In the directory of the materials there is a template of `letters.py` which contains the statistic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbacd67f-2f72-4da8-a45a-572e938e5112",
   "metadata": {},
   "source": [
    "### 3.2 Read the text file (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1466d7cd-440c-411c-aeaa-a3db237d761f",
   "metadata": {},
   "source": [
    "Start with the text-file `test01.txt` as a development example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745bb250-15ed-488f-beb6-207b4b80f98a",
   "metadata": {},
   "source": [
    "**Your task:**\n",
    "\n",
    "Define a function `read_file_to_letters` in the modul `letters` which takes a filename as an argument. It should return the letter frequency  of the data with the given filename."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344dd092-fba7-48cc-abe9-4419e206f75e",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    " * for the letter frequency please use a python dictionary\n",
    " * we need to analyse all characters from `a` to `z` (no special characters or punctuation or spaces, all characters are converted to lower characters)\n",
    " * the values in the dictionary should be scaled down to a percentage number between `0` and `1`\n",
    " * call the function from the main script, import the module with importing the statistic definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9141042b-b400-48e8-91aa-978ec6f3bab7",
   "metadata": {},
   "source": [
    "### 3.3 Compare the languages (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc4e51-9ad6-427b-9991-3157fc669e47",
   "metadata": {},
   "source": [
    "The next task is to compare your letter frequency with some predefined statistics and decide how well your data fits. You can simply check for each given letter the distance `d` from your value with the given value. The value `1-d` will then give you a fit for an individual letter. For the sequence of letters the minimum of all individual `1-d` values will then describe how good a language will fit to the data. The larger this value will be, so higher is the probability that the letter frequency fits the predefined data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5aa7b-05f4-4b1c-8d25-7b5b4f7b5a29",
   "metadata": {},
   "source": [
    "**Your task:**\n",
    "\n",
    "Define a function `test_language` in the module `letters` which takes the your letter frequency and a predefined statistic as arguments and returns the minimum value `1-d` for all individual letters in the predefined statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6578e-e16c-4ee9-a875-475021b5bf9b",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    " * use __only__ the letters defined in the predefined statistics for the analysis\n",
    " * call the function from the main script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b26f90-bab7-4637-a7c3-92d387757caf",
   "metadata": {},
   "source": [
    "### 3.4 Decision of languages (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0423efe9-604e-461d-9100-a1ad9ff08550",
   "metadata": {},
   "source": [
    "Based on the previous tasks, we need now to decide which language fits best. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74debb8-717d-4b4f-b368-4d979af42602",
   "metadata": {},
   "source": [
    "**Your task:**\n",
    "\n",
    "Define a function `decide_language` in the module `letters` which takes your letter frequency and returns a language name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ade19a-662c-4e0f-aa92-5a9dee5517cb",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    " * call the function from the main script\n",
    " * `test_language` is not needed any more for your main script, remove the `import` of this function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2770d05-327d-4790-93cc-9e7611ea5548",
   "metadata": {},
   "source": [
    "### 3.5 Batch-check (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddfc785-20b9-40b6-9199-c91e023b378b",
   "metadata": {},
   "source": [
    "**Your task:**\n",
    "\n",
    "Check all given files in `texts/` for the used languges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342d9c24-35ba-4ef7-a69c-c3751921e519",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    "\n",
    "For all of you who are not working on the *Arctic Ice map*-Project: You can get all files from a specific directory back in a python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb010cd-44de-483a-acf4-f80377519301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The glob-module allows you to use Linux style\n",
    "# pathname expansion\n",
    "import glob\n",
    "\n",
    "# generate a list of files matching the Unix-pattern\n",
    "# texts/*.txt. \n",
    "datapath = \"texts\"\n",
    "filelist = glob.glob(f\"{datapath}/*.txt\")\n",
    "\n",
    "# print the resulst\n",
    "print(filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c61074b-246d-4087-b471-ec3e9825c4a6",
   "metadata": {},
   "source": [
    "### 3.6 Bonus/Fun (0 points)\n",
    "\n",
    "If you manually checks the languages of the files, the algorithm works nicely, except for `texts/test06.txt`. The language is english, but my tests returns `german` as the used language. Probably, my english is not the best, I know, but is this also a hint that we can detect not natively speaking persons? Please check yourself how this is working for you and send me a private feedback!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a51d5-a624-42cb-b107-acc7c8895e82",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
